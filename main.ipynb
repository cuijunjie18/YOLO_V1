{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e7a72c",
   "metadata": {},
   "source": [
    "### **yolov1论文**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d1dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795a34d",
   "metadata": {},
   "source": [
    "**一、搭建模型**\n",
    "\n",
    "**backbone采用VGG架构**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e84419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolov1(num_classes = 20,num_bboxes = 2):\n",
    "    \"\"\"获取yolov1o模型\"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(3,64,kernel_size = 7,stride = 2,padding = 3),nn.LeakyReLU(),\n",
    "        nn.MaxPool2d(2,2),                    # k = 2,s = 2的MaxPool2d层使图像分辨率减半\n",
    "        nn.Conv2d(64,192,kernel_size = 3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        nn.Conv2d(192,128,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(128,256,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,256,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,512,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        nn.Conv2d(512,256,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,512,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,256,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,512,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,256,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,512,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,256,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(256,512,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,512,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "        nn.Conv2d(1024,512,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(1024,512,1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(512,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(1024,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(1024,1024,3,stride = 2,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(1024,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Conv2d(1024,1024,3,padding = 1),nn.LeakyReLU(),\n",
    "        nn.Flatten(),nn.Linear(7 * 7 * 1024,4096),\n",
    "        nn.Linear(4096,7 * 7 * (num_bboxes * 5 + num_classes))\n",
    "    )\n",
    "\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self,num_classes = 20,num_bboxes = 2):\n",
    "        super().__init__()\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "        self.layer = get_yolov1(self.C,self.B)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X = self.layer(X)\n",
    "        X = X.reshape(X.shape[0],self.B * 5 + \n",
    "                      self.C,7,7)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba188d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Yolov1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a77355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand((1,3,448,448)) # 通道优先\n",
    "y_pred = net(input).reshape(1,30,7,7)\n",
    "# y_pred = net(input)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a7f2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "yolov1                                   [1, 30, 7, 7]             --\n",
      "├─Sequential: 1-1                        [1, 1470]                 --\n",
      "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         9,472\n",
      "│    └─LeakyReLU: 2-2                    [1, 64, 224, 224]         --\n",
      "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
      "│    └─Conv2d: 2-4                       [1, 192, 112, 112]        110,784\n",
      "│    └─LeakyReLU: 2-5                    [1, 192, 112, 112]        --\n",
      "│    └─MaxPool2d: 2-6                    [1, 192, 56, 56]          --\n",
      "│    └─Conv2d: 2-7                       [1, 128, 56, 56]          24,704\n",
      "│    └─LeakyReLU: 2-8                    [1, 128, 56, 56]          --\n",
      "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          295,168\n",
      "│    └─LeakyReLU: 2-10                   [1, 256, 56, 56]          --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          65,792\n",
      "│    └─LeakyReLU: 2-12                   [1, 256, 56, 56]          --\n",
      "│    └─Conv2d: 2-13                      [1, 512, 56, 56]          1,180,160\n",
      "│    └─LeakyReLU: 2-14                   [1, 512, 56, 56]          --\n",
      "│    └─MaxPool2d: 2-15                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-16                      [1, 256, 28, 28]          131,328\n",
      "│    └─LeakyReLU: 2-17                   [1, 256, 28, 28]          --\n",
      "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
      "│    └─LeakyReLU: 2-19                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-20                      [1, 256, 28, 28]          131,328\n",
      "│    └─LeakyReLU: 2-21                   [1, 256, 28, 28]          --\n",
      "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          1,180,160\n",
      "│    └─LeakyReLU: 2-23                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-24                      [1, 256, 28, 28]          131,328\n",
      "│    └─LeakyReLU: 2-25                   [1, 256, 28, 28]          --\n",
      "│    └─Conv2d: 2-26                      [1, 512, 28, 28]          1,180,160\n",
      "│    └─LeakyReLU: 2-27                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-28                      [1, 256, 28, 28]          131,328\n",
      "│    └─LeakyReLU: 2-29                   [1, 256, 28, 28]          --\n",
      "│    └─Conv2d: 2-30                      [1, 512, 28, 28]          1,180,160\n",
      "│    └─LeakyReLU: 2-31                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-32                      [1, 512, 28, 28]          262,656\n",
      "│    └─LeakyReLU: 2-33                   [1, 512, 28, 28]          --\n",
      "│    └─Conv2d: 2-34                      [1, 1024, 28, 28]         4,719,616\n",
      "│    └─LeakyReLU: 2-35                   [1, 1024, 28, 28]         --\n",
      "│    └─MaxPool2d: 2-36                   [1, 1024, 14, 14]         --\n",
      "│    └─Conv2d: 2-37                      [1, 512, 14, 14]          524,800\n",
      "│    └─LeakyReLU: 2-38                   [1, 512, 14, 14]          --\n",
      "│    └─Conv2d: 2-39                      [1, 1024, 14, 14]         4,719,616\n",
      "│    └─LeakyReLU: 2-40                   [1, 1024, 14, 14]         --\n",
      "│    └─Conv2d: 2-41                      [1, 512, 14, 14]          524,800\n",
      "│    └─LeakyReLU: 2-42                   [1, 512, 14, 14]          --\n",
      "│    └─Conv2d: 2-43                      [1, 1024, 14, 14]         4,719,616\n",
      "│    └─LeakyReLU: 2-44                   [1, 1024, 14, 14]         --\n",
      "│    └─Conv2d: 2-45                      [1, 1024, 14, 14]         9,438,208\n",
      "│    └─LeakyReLU: 2-46                   [1, 1024, 14, 14]         --\n",
      "│    └─Conv2d: 2-47                      [1, 1024, 7, 7]           9,438,208\n",
      "│    └─LeakyReLU: 2-48                   [1, 1024, 7, 7]           --\n",
      "│    └─Conv2d: 2-49                      [1, 1024, 7, 7]           9,438,208\n",
      "│    └─LeakyReLU: 2-50                   [1, 1024, 7, 7]           --\n",
      "│    └─Conv2d: 2-51                      [1, 1024, 7, 7]           9,438,208\n",
      "│    └─LeakyReLU: 2-52                   [1, 1024, 7, 7]           --\n",
      "│    └─Flatten: 2-53                     [1, 50176]                --\n",
      "│    └─Linear: 2-54                      [1, 4096]                 205,524,992\n",
      "│    └─Linear: 2-55                      [1, 1470]                 6,022,590\n",
      "==========================================================================================\n",
      "Total params: 271,703,550\n",
      "Trainable params: 271,703,550\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 20.30\n",
      "==========================================================================================\n",
      "Input size (MB): 2.41\n",
      "Forward/backward pass size (MB): 110.43\n",
      "Params size (MB): 1086.81\n",
      "Estimated Total Size (MB): 1199.65\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(net,input_data = input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550eae18",
   "metadata": {},
   "source": [
    "**二、定义损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6390813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"定义一个为yolov1的损失函数\"\"\"\n",
    "\n",
    "    def __init__(self,feature_size=7, num_bboxes=2, num_classes=20, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        super().__init__()\n",
    "        self.S = feature_size\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def compute_iou(self,bbox1,bbox2):\n",
    "        \"\"\"\"\n",
    "        计算两组边界框之间的交并比(IoU)\n",
    "        \n",
    "        参数:\n",
    "        - bbox1: 形状为 [N, 4] 的边界框 (xmin, ymin, xmax, ymax)\n",
    "        - bbox2: 形状为 [M, 4] 的边界框 (xmin, ymin, xmax, ymax)\n",
    "        \n",
    "        返回:\n",
    "        - iou: 形状为 [N, M] 的 IoU 矩阵\n",
    "        \"\"\"\n",
    "        # 获取边界框数量\n",
    "        N = bbox1.size(0)\n",
    "        M = bbox2.size(0)\n",
    "        \n",
    "        # 计算交集的左上角坐标 (left-top)\n",
    "        lt = torch.max(\n",
    "            bbox1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, :2].unsqueeze(0).expand(N, M, 2)   # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        \n",
    "        # 计算交集的右下角坐标 (right-bottom)\n",
    "        rb = torch.min(\n",
    "            bbox1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N, 2] -> [N, 1, 2] -> [N, M, 2]\n",
    "            bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)    # [M, 2] -> [1, M, 2] -> [N, M, 2]\n",
    "        )\n",
    "        \n",
    "        # 计算交集的宽高\n",
    "        wh = rb - lt\n",
    "        wh[wh < 0] = 0  # 处理无重叠的情况\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1]  # 交集面积 [N, M]\n",
    "        \n",
    "        # 计算两个边界框各自的面积\n",
    "        area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1])  # [N, ]\n",
    "        area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1])  # [M, ]\n",
    "        area1 = area1.unsqueeze(1).expand_as(inter)  # [N, ] -> [N, 1] -> [N, M]\n",
    "        area2 = area2.unsqueeze(0).expand_as(inter)  # [M, ] -> [1, M] -> [N, M]\n",
    "        \n",
    "        # 计算并集面积\n",
    "        union = area1 + area2 - inter  # [N, M]\n",
    "        \n",
    "        # 计算 IoU\n",
    "        iou = inter / union  # [N, M]\n",
    "        \n",
    "        return iou\n",
    "\n",
    "\n",
    "    def forward(self,pred:torch.Tensor,target:torch.Tensor):\n",
    "        \"\"\"\n",
    "        计算 YOLOv1 损失\n",
    "        \n",
    "        参数:\n",
    "        - pred_tensor: 模型预测的输出张量，形状为 [batch_size, S, S, B*5 + C]\n",
    "        - target_tensor: 目标标签张量，形状与 pred_tensor 相同\n",
    "        \n",
    "        返回:\n",
    "        - loss: 计算得到的损失值\n",
    "        \"\"\"\n",
    "        # target/pred = (N,C,H,W) -> (N,H,W,C)\n",
    "        target = target.permute(0,2,3,1)\n",
    "        pred = pred.permute(0,2,3,1)\n",
    "        batch_size = pred.shape[0]\n",
    "\n",
    "        # 设置临时参数，减少重复self引用\n",
    "        S = self.S\n",
    "        B = self.B\n",
    "        C = self.C\n",
    "        grid_size = 1.0 / S # 归一化的网格大小\n",
    "\n",
    "        # 设置有目标的mask和没目标的mask\n",
    "        coord_mask = target[:,:,:,4] > 0\n",
    "        noobj_mask = target[:,:,:,4] == 0\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand_as(target)\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target)\n",
    "\n",
    "        # 提取有目标的pred和没目标的pred\n",
    "        coord_pred = pred[coord_mask].reshape(-1, 5 * B + C)\n",
    "        noobj_pred = pred[noobj_mask].reshape(-1, 5 * B + C)\n",
    "\n",
    "        # 提取有目标的target和没目标的target\n",
    "        coord_target = target[coord_mask].reshape(-1, 5 * B + C)\n",
    "        noobj_target = target[noobj_mask].reshape(-1, 5 * B + C)\n",
    "\n",
    "        # 提取bbox与class\n",
    "        bbox_pred = coord_pred[:,:5 * B].reshape(-1, 5)\n",
    "        class_pred = coord_pred[:,5 * B:]\n",
    "        bbox_target = coord_target[:,:5 * B].reshape(-1, 5)\n",
    "        class_target = coord_target[:,5 * B:]\n",
    "\n",
    "        # 处理无目标位置的置信度损失\n",
    "        noobj_conf_mask = torch.BoolTensor(noobj_pred.shape).fill_(0)\n",
    "        for b in range(B):\n",
    "            noobj_conf_mask[:,4 + b * 5] = 1 # 设置提取出置信度的位置\n",
    "        noobj_conf_pred = noobj_pred[noobj_conf_mask]\n",
    "        noonj_conf_target = noobj_target[noobj_conf_mask]\n",
    "\n",
    "        # 计算noobj_loss_conf\n",
    "        loss_conf_noobj = F.mse_loss(noobj_conf_pred,noonj_conf_target,reduction = 'sum')\n",
    "\n",
    "        # 初始化响应掩码\n",
    "        coord_response_mask = torch.BoolTensor(bbox_target.size()).fill_(0) # 响应初始化为0\n",
    "        coord_not_response_mask = torch.BoolTensor(bbox_target.size()).fill_(1) # 非响应初始化为1\n",
    "        bbox_target_iou = torch.zeros(bbox_target.size())\n",
    "\n",
    "        # 遍历每个目标网格\n",
    "        for i in range(0,bbox_pred.shape[0],B):\n",
    "            # 获取当前网格的 B 个预测边界框\n",
    "            pred = bbox_pred[i:i + B] \n",
    "\n",
    "            # 将预测边界框转换为 (xmin, ymin, xmax, ymax) 格式\n",
    "            pred_xyxy = torch.zeros((pred.shape[0],4))\n",
    "            pred_xyxy[:,:2] = pred[:,:2] * grid_size - pred[:,2:4] * 0.5    # 左上\n",
    "            pred_xyxy[:,2:4] = pred[:,:2] * grid_size + pred[:,2:4] * 0.5   # 右下\n",
    "\n",
    "            # 同样处理出目标坐标\n",
    "            target = bbox_target[i].reshape(-1,5)\n",
    "            target_xyxy = torch.zeros((target.shape[0],4))\n",
    "            target_xyxy[:,:2] = target[:,:2] * grid_size - target[:,2:4] * 0.5    # 左上\n",
    "            target_xyxy[:,2:4] = target[:,:2] * grid_size + target[:,2:4] * 0.5   # 右下\n",
    "\n",
    "            # 计算iou\n",
    "            iou = self.compute_iou(pred_xyxy,target_xyxy)\n",
    "            max_iou, max_index = iou.max(0)  # 找出最大 IoU 及其索引\n",
    "\n",
    "            # 标记负责预测目标的边界框\n",
    "            coord_response_mask[i + max_index] = 1\n",
    "            coord_not_response_mask[i + max_index] = 0\n",
    "\n",
    "            # 将最大 IoU 作为置信度目标\n",
    "            bbox_target_iou[i + max_index,4] = max_iou.data\n",
    "\n",
    "        # 提取负责预测目标的边界框\n",
    "        bbox_pred_response = bbox_pred[coord_response_mask].reshape(-1, 5)\n",
    "        bbox_target_response = bbox_target[coord_response_mask].reshape(-1, 5)\n",
    "        target_iou = bbox_target_iou[coord_response_mask].reshape(-1, 5)\n",
    "\n",
    "        # 计算坐标损失 (中心点坐标)\n",
    "        loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
    "        \n",
    "        # 计算宽高损失\n",
    "        loss_wh = F.mse_loss(\n",
    "            torch.sqrt(bbox_pred_response[:,2:4]), \n",
    "            torch.sqrt(bbox_target_response[:,2:4]), \n",
    "            reduction = 'sum'\n",
    "        )\n",
    "\n",
    "        # 计算目标置信度损失\n",
    "        loss_conf = F.mse_loss(bbox_pred_response[:,4], target_iou[:,4], reduction = 'sum')\n",
    "\n",
    "        # 计算类别预测损失\n",
    "        loss_class = F.mse_loss(class_pred,class_target,reduction = 'sum')\n",
    "\n",
    "        loss = (self.lambda_coord * (loss_xy + loss_wh) + \n",
    "                self.lambda_noobj * loss_conf_noobj +\n",
    "                loss_conf + loss_class)\n",
    "        \n",
    "        return loss / batch_size # 平均损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867dc26a",
   "metadata": {},
   "source": [
    "**三、训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f1c9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "from utils.datasets import YoloData\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from utils.net_frame import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d65aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract dataset: 100%|██████████| 13700/13700 [00:27<00:00, 503.39it/s]\n",
      "Normalize img: 100%|██████████| 13700/13700 [00:15<00:00, 878.40it/s] \n",
      "Normalize bboxes: 100%|██████████| 13700/13700 [00:00<00:00, 111983.90it/s]\n",
      "Generating targets: 100%|██████████| 13700/13700 [00:03<00:00, 3849.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义图像transforms\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Normalize(\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std  = [0.229, 0.224, 0.225]\n",
    "    ),torchvision.transforms.Resize((448,448))]\n",
    ")\n",
    "# 加载数据集\n",
    "yolodata = YoloData(\"datasets/JPEGImages\",\"datasets/train.txt\",transforms = transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbeb0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter = data.DataLoader(yolodata,batch_size = 128,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8c75352",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Yolov1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "999888f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "loss = YoloLoss()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffc96e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 定义训练函数\n",
    "def train(net,trainer,train_iter,test_iter,loss_fn,lr,num_epochs,devices_idx = None):\n",
    "    \"\"\"训练情感分析模型\"\"\"\n",
    "    # 设置设备\n",
    "    if devices_idx == None:\n",
    "        device = try_gpu(i = 0)\n",
    "    else:\n",
    "        assert (type(devices_idx == list) and \n",
    "                type(devices_idx[0]) == int),\"devices_idx must be list of int\"\n",
    "        devices = [torch.device(f\"cuda:{i}\")\n",
    "                   for i in devices_idx]\n",
    "    print(f\"Training on{devices}\")\n",
    "    \n",
    "    # 多GPU加载网络(当len(devices) == 1时即单卡训练)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    # 开始训练\n",
    "    loss_plt = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train() # 循环涉及评估，则每次循环前要net.train()\n",
    "        loop = tqdm(train_iter,desc = f\"Epoch:[{epoch + 1}/{num_epochs}]\",\n",
    "                    total = len(train_iter))\n",
    "        loss_temp = 0\n",
    "        total_nums = 0\n",
    "        for batch in loop:\n",
    "            # 清空梯度\n",
    "            trainer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            X,Y = batch\n",
    "            if type(X) == list:\n",
    "                X = [x.to(devices[0]) for x in X]\n",
    "                total_nums += X[0].shape[0]\n",
    "            else:\n",
    "                X = X.to(devices[0]) # 放置在devices[0]即可\n",
    "                total_nums += X.shape[0]\n",
    "            Y = Y.to(devices[0])\n",
    "            # print(X.shape,Y.shape)\n",
    "            y_pred = net(X)\n",
    "\n",
    "            # count loss and backwar\n",
    "            loss = loss_fn(y_pred,Y)\n",
    "            loss.sum().backward()\n",
    "            trainer.step()\n",
    "\n",
    "            # 先step后再调用item()，否则切断计算图\n",
    "            loss_temp += loss.sum().item()\n",
    "            \n",
    "            # # update parameters\n",
    "            # trainer.step()\n",
    "            loop.set_postfix({\"LOSS\" : loss_temp / total_nums,\"lr\" : \"{:e}\".format(trainer.param_groups[0]['lr'])})\n",
    "        loss_plt.append(loss_temp / total_nums)\n",
    "    return loss_plt,train_accs,test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cac9d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on[device(type='cuda', index=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:[1/20]:   0%|          | 0/108 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevices_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainer, train_iter, test_iter, loss_fn, lr, num_epochs, devices_idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m net(X)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# count loss and backwar\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     48\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/data_all/cjj_node/local/anaconda3/envs/QwenLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data_all/cjj_node/local/anaconda3/envs/QwenLM/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 160\u001b[0m, in \u001b[0;36mYoloLoss.forward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m    153\u001b[0m loss_wh \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m    154\u001b[0m     torch\u001b[38;5;241m.\u001b[39msqrt(bbox_pred_response[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]), \n\u001b[1;32m    155\u001b[0m     torch\u001b[38;5;241m.\u001b[39msqrt(bbox_target_response[:,\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m]), \n\u001b[1;32m    156\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# 计算目标置信度损失\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m loss_conf \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_pred_response\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_iou\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# 计算类别预测损失\u001b[39;00m\n\u001b[1;32m    163\u001b[0m loss_class \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(class_pred,class_target,reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data_all/cjj_node/local/anaconda3/envs/QwenLM/lib/python3.10/site-packages/torch/nn/functional.py:3384\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3381\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3383\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "plt_loss = train(net,trainer,train_iter,test_iter = None,loss_fn = loss,\n",
    "                 num_epochs = 20,lr = lr,devices_idx = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc1cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QwenLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
